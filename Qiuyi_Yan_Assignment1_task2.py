# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZrSkUiFOAywNXMG8Lfo2Ikd0WqjDAWCA
"""

#!pip install --ignore-installed -q pyspark==3.2.1

import os
import sys
import requests
from operator import add

from pyspark import SparkConf,SparkContext
from pyspark.streaming import StreamingContext

from pyspark.sql import SparkSession
from pyspark.sql import SQLContext

from pyspark.sql.types import *
from pyspark.sql import functions as func
from pyspark.sql.functions import *

spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = SparkContext.getOrCreate()
sqlContext = SQLContext(sc)

def isfloat(value):
    try:
        float(value)
        return True

    except:
         return False

#Function - Cleaning
#For example, remove lines if they don’t have 16 values and
# checking if the trip distance and fare amount is a float number
# checking if the trip duration is more than a minute, trip distance is more than 0.1 miles,
# fare amount and total amount are more than 0 dollars
def correctRows(p):
    if(len(p)==17):
        if(isfloat(p[5]) and isfloat(p[11])):
            if(float(p[4])> 60 and float(p[5])>0 and float(p[11])> 0 and float(p[16])> 0):
                return p

# Upload the data
# At this point you have to upload your data to google colab
#!wget -O /content/taxi-data-sorted-verysmall.csv https://storage.googleapis.com/met-cs-777-data/taxi-data-sorted-verysmall.csv

#pwd

#rdd = sc.textFile(sys.argv[1])
#print(rdd)

# Set your file path here
#path="file:///content/"

# You have 1 files

testFile =  sys.argv[1]


testDataFrame = sqlContext.read.format('csv').options(header='false', inferSchema='true',  sep =",").load(testFile)

#testDataFrame.show()

testRDD = testDataFrame.rdd.map(tuple)
testRDD.take(1)

# calling isfloat and correctRows functions to cleaning up data
taxilinesCorrected = testRDD.filter(correctRows)
taxilinesCorrected.take(2)



"""#task1"""

#rdd1=taxilinesCorrected.map(lambda p: (p[0],p[1]) )
#print(rdd1.collect())

#rdd2 = rdd1.map(lambda x: (x,1))
#print(rdd2.collect())

#rdd3 = rdd2.reduceByKey(lambda x, y : x+y)
#print(rdd2.collect())

#rdd4 = rdd3.map(lambda p: (p[0][0],p[1]))
#print(rdd4.collect())

#top = rdd4.top(10, lambda x:x[1])
#print(top)



"""#task2"""

rdd21=taxilinesCorrected.map(lambda p: (p[1],p[4],p[5],p[15],p[16]) )
#print(rdd21.collect())

rdd22=taxilinesCorrected.map(lambda p: (p[1],(p[16]-p[15])/(p[4]/60)) )
#print(rdd22.collect())

rdd23 = rdd22.reduceByKey(lambda x, y : (x+y)/2)
#print(rdd23.collect())

top2 = rdd23.top(10, lambda x:x[1])
#print(top2)

#results_1 = top
#print(type(results_1))
results_2 = top2
print(type(results_2))

from pyspark import SparkContext

sc = SparkContext.getOrCreate()
#data1 = results_1

# 将 list 转换为 RDD
#rdd00 = sc.parallelize(data1)
#results_1 = rdd00
#print(type(results_1))



# 假设你的数据是这样的
data2 = results_2

# 将 list 转换为 RDD
rdd02 = sc.parallelize(data2)
results_2 = rdd02
print(type(results_2))
#results_1.coalesce(1).saveAsTextFile(sys.argv[2])
results_2.coalesce(1).saveAsTextFile(sys.argv[2])
sc.stop()